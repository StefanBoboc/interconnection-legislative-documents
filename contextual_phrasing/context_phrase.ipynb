{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b9b5a-c3a8-4a64-bbfd-24beaa08dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Using file doc_id_len_sorted.json, each document is taken, the content of the current document is read, \n",
    "# and the mentions belonging to it are extracted from the database. \n",
    "#\n",
    "# The content of the document is separated into sentences using spacy.nlp, and for each mention, the sentence it belongs to is selected.\n",
    "# At the same time, the indices from which each mention starts in the identified sentence are calculated.\n",
    "#\n",
    "# Save the generated DataFrame in gpt_dataset_<no.>.json\n",
    "#\n",
    "\n",
    "import mysql.connector as mysql\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "def main():\n",
    "    # Establish a single connection to the database\n",
    "    mydb = mysql.connect(\n",
    "        host=\"127.0.0.1\",\n",
    "        user=\"root\",\n",
    "        passwd=\"xxxxx\",\n",
    "        database=\"preprocessed\"\n",
    "    )\n",
    "\n",
    "    nlp = spacy.load(\"ro_core_news_lg\")\n",
    "    nlp.max_length = 1500000\n",
    "\n",
    "    with open('doc_id_len_sorted.json', 'r') as file:\n",
    "        loaded_list_file = json.load(file)\n",
    "\n",
    "    # Running in batches\n",
    "    filtered_dict = {k: v for k, v in loaded_list_file.items() if v <= 1000000}\n",
    "    # 1st run: sorted_filtered_items = sorted(filtered_dict.items(), key=lambda item: item[1])[:10000]\n",
    "    # 2nd run: sorted_filtered_items = sorted(filtered_dict.items(), key=lambda item: item[1])[10000:20000]\n",
    "    # 3rd run: sorted_filtered_items = sorted(filtered_dict.items(), key=lambda item: item[1])[20000:30000]\n",
    "    # 4th run: sorted_filtered_items = sorted(filtered_dict.items(), key=lambda item: item[1])[30000:40000]\n",
    "    # 5th run: sorted_filtered_items = sorted(filtered_dict.items(), key=lambda item: item[1])[40000:50000]\n",
    "    sorted_filtered_items = sorted(filtered_dict.items(), key=lambda item: item[1])[50000:60000]\n",
    "    doc2process = dict(sorted_filtered_items)\n",
    "    \n",
    "    for doc_id, doc_len in doc2process.items():\n",
    "        # Document with the doc_id\n",
    "        query = f\"SELECT * FROM unique_document WHERE doc_id = {doc_id}\"\n",
    "        df_document = pd.read_sql_query(query, con=mydb);\n",
    "\n",
    "        if df_document.empty:\n",
    "            print(f\"Doc_id: {doc_id} is empty\")\n",
    "            continue\n",
    "\n",
    "        # Mentions from doc_id document\n",
    "        query = f\"SELECT * FROM unique_mention WHERE doc_id = {doc_id}\"\n",
    "        df_mention = pd.read_sql_query(query, con=mydb);\n",
    "\n",
    "        if df_mention.empty:\n",
    "            print(f\"Mentions from doc_id: {doc_id} document are empty\")\n",
    "            continue\n",
    "\n",
    "        # Read file with the content of the document\n",
    "        with open(df_document.at[0, 'doc_content'], 'r', encoding='utf-8') as file:\n",
    "            doc_content = file.read()\n",
    "\n",
    "        # Tokenize document content\n",
    "        doc_nlp = nlp(doc_content)\n",
    "\n",
    "        sent_text_list = []\n",
    "        sent_start_list = []\n",
    "        sent_end_list = []\n",
    "\n",
    "        sent_start = 0\n",
    "        for sent in doc_nlp.sents:\n",
    "            sent_len = len(sent.text) + 1\n",
    "            sent_end = sent_start + sent_len - 1\n",
    "\n",
    "            sent_text_list.append(sent.text)\n",
    "            sent_start_list.append(sent_start)\n",
    "            sent_end_list.append(sent_end)\n",
    "\n",
    "            sent_start = sent_end + 1\n",
    "\n",
    "        # Determine sentence boundaries for each mention\n",
    "        mention_ends = df_mention['men_end'].values\n",
    "        mention_starts = df_mention['men_start'].values\n",
    "\n",
    "        end_interval = [next(i for i in sent_end_list if i >= end_date) for end_date in mention_ends]\n",
    "        start_interval = [next(i for i in reversed(sent_start_list) if i <= start_date) for start_date in mention_starts]\n",
    "\n",
    "        doc_context_list = [\n",
    "            ' '.join(sent_text_list[sent_start_list.index(start):sent_end_list.index(end) + 1])\n",
    "            for start, end in zip(start_interval, end_interval)\n",
    "        ]\n",
    "        \n",
    "        df_mention['men_start_in_phrase'] = np.array(mention_starts) - np.array(start_interval)\n",
    "        df_mention['phrase'] = doc_context_list\n",
    "        df_mention['doc_title'] = df_document.at[0, 'doc_title']\n",
    "        df_mention['doc_url'] = df_document.at[0, 'doc_url']\n",
    "        df_mention['doc_content'] = df_document.at[0, 'doc_content']\n",
    "\n",
    "        df_mention = df_mention.rename(columns={'men_start': 'men_start_in_doc', 'men_end': 'men_end_in_doc'})\n",
    "\n",
    "        # Convert DataFrame to JSON and write to file\n",
    "        json_str = df_mention.to_json(orient='records', lines=True, force_ascii=False)\n",
    "        with open(r'C:\\Users\\xxxxx\\Desktop\\interconnection_legislative_documents\\01_phrase_selection\\phrasing001\\gpt_dataset_6.json', 'a', encoding='utf-8') as file:\n",
    "            file.write(json_str)\n",
    "\n",
    "        del df_mention\n",
    "        del df_document\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings('ignore')\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbfd81-a653-4390-a6ad-977ed3f4032e",
   "metadata": {},
   "source": [
    "{'31855': 148, '18361': 226, '24784': 227, '26803': 234, '12778': 241, '24396': 242, '7403': 245, '24778': 249, '27924': 250, '34309': 252}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
